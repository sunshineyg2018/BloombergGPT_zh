# 1. 背景
: A Large Language Model for Finance中涉及到的语言模型，也是金融领域第一个公开发表文章的大语言模型（以下简称“LLM”）。
此项目是对此篇论文的技术复现
BloombergGPT是布隆伯格2023年3月30日公开在arXiv的一篇文章——BloombergGPT

原论文地址:https://arxiv.org/abs/2303.17564

# 2. 要点
* BloombergGPT是Bloomberg训练出来的金融大语言模型（LLM for Finance）
* 模型参数量为500亿，使用了包含3630亿token的金融领域数据集以及3450亿token的通用数据集
* 隐藏层维度为7680，多头的头数为40
* 模型采用Unigram tokenizer，AdamW优化器
* 模型在64个AWS的p4d.24xlarge实例上训练了53天，其中每个p4d.24xlarge实例包含了8块40GB的A100GPU
* 对BloombergGPT的评估包含了两部分：金融领域评估与通用领域评估
* 评估对比的其他大语言模型有GPT-NeoX、OPT、BLOOM、GPT-3
* 在金融领域任务上，BloombergGPT综合表现最好；在通用任务上，BloombergGPT的综合得分同样优于相同参数量级的其他模型，并且在某些任务上的得分要高于参数量更大的模型
* BloombergGPT模型在金融领域取得好效果的同时，并没有以牺牲模型通用能力为代价
* 对模型定性评估的结果表明，BloombergGPT可以提高工作效率
* 出于安全性的考虑，BloogbergGPT模型不会被公开，但是模型训练和评估的相关经验和思考会被分享出来
* 作者认为，对模型效果提升促进最大的三个因素（按影响从高到低排序）分别为精心清洗的数据集、合理的tokenizer、流行的模型结构

# 2. 引言
BloombergGPT是一个有500亿参数、基于BLOOM模型的LLM，过程中采用了一种兼具通用能力和特定领域的方法。

为了使用这种方法，作者们基于Bloomberg 40年来积累的数据构造了目前最大的金融领域数据集。

文章的主要贡献在以下几点：

* 混合数据集训练方法不仅可以在特定任务上表现出色，也可以在一般NLP基准测试上表现良好
* 不同于常见的网络爬取数据，本文的数据包含了巨量的可信来源的精心清洗的数据
* 不仅包含了模型在基准测试集上的评估结果，也包含了在Bloomberg内部任务上的评估结果
* 在超过7000亿个token的语料库中的5690亿个token上训练出一个500亿参数的LLM
* 使用Unigram模型而非常用的基于贪心合并的子词标记器进行tokenize，方便在推理时进行更智能的标记化
* 借鉴BLOOM的训练大模型方法，同时也将自己自己在训练BloombergGPT中的经验分享


#3. 数据集

BloombergGPT是一个有500亿参数、基于BLOOM模型的LLM，过程中采用了一种兼具通用能力和特定领域的方法。
参照原作者FinPile选项勾构建了一个包含了新闻、档案、网络爬取的新闻稿件、英文财经文档等英文金融文档的金融领域数据集，
数据集暂不予公开


#3. 模型下载

